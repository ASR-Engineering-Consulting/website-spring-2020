---
# Page settings
layout: default
keywords:
comments: false

# Hero section
title: Week 6 - No Section
description: Midterm Week (No Section)


# Micro navigation
micro_nav: true

---

No section this week, good luck on the midterm!

<!--

# Regularization in Code

### Data Augmentation

**TensorFlow**: [documentation](https://www.tensorflow.org/api_docs/python/tf/image/)
```python
# Resizing
tf_resize = tf.image.resize_images(
    images,
    size,
    method=ResizeMethod.BILINEAR,
    align_corners=False,
    preserve_aspect_ratio=False
)
# Scaling
tf_scaling = tf.image.crop_and_resize(
    image,
    boxes,
    box_ind,
    crop_size,
    method='bilinear',
    extrapolation_value=0,
    name=None
)
# Translation
tf_translation = tf.image.extract_glimpse(
    input,
    size,
    offsets,
    centered=True,
    normalized=True,
    uniform_noise=True,
    name=None
)
# Rotation
tf_90rotation = tf.image.rot90(
    image,
    k=1,
    name=None
)
# Execute Transformation
sess.run(transformation, feed_dict = {X: imgs})
```

### Early Stopping

Early stopping can be implemented directly in Python.

```python
def early_stopping(model, X_train, Y_train, X_dev, Y_dev, eval_freq = 1, stop_wait = 100):
    """
    Python implementation of the early stopping algorithm.

    Arguments:
    model -- Your model class which comes with functions:
                model.train() -- trains the model for one step and returns the updated model.
                model.eval() -- evaluates the model on a set and returns the error.
                model.copy() -- copies the model.
    X_train -- Training set
    Y_train -- Training labels
    X_dev -- Dev set
    Y_dev -- Dev labels
    eval_freq -- integer, ratio of training step per evaluation step.
    stop_wait -- number of train step without dev error improvement before stopping.

    Returns:
    best_model -- the model with minimal dev error and early stopping
    best_dev_error -- the best dev error achieved by best_model
    best_model_index -- the best_model's saving step 
    """

    current_model = model.copy()       # Track your current model
    best_model = model.copy()          # Track your best model
    num_train_steps = 0                # number of training steps
    num_steps_since_best_model = 0     # number of training steps since the last best model was saved
    best_dev_error = np.inf            # the best dev error achieved so far (by best_model)
    best_model_index = 0               # the best_model's saving step 

    # Loop unless there's no model improvement on the dev set for stop_wait steps.
    while num_steps_since_best_model < stop_wait:

        # Train current_model for "train_per_eval_ratio" steps
        for s in range(train_per_eval_ratio):
            current_model.train(X_train, Y_train)
            num_train_steps = num_train_steps + 1

        # Evaluate your current_model on the dev set.
        dev_error = current_model.eval(X_dev, Y_dev)

        # If the dev error is lower than the best dev error previously achieved, then save the current_model as the best_model
        if dev_error < best_dev_error:
            num_steps_since_best_model = 0
            best_model = current_model.copy()
            best_model_index = num_train_steps
            current_model = best_model
        # Otherwise increment num_steps_since_best_model
        else:
            num_steps_since_best_model = num_steps_since_best_model + 1

  return best_model, best_dev_error, best_model_index
```

### L1 and L2 Regularization

In TensorFlow L1/L2 regularization terms are added directly to the loss function.

**TensorFlow**: [documentation]()
```python
# L2 Regularization
regularization_terms = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)
total_loss = tf.reduce_mean(loss + beta * regularization_terms)
optimizer = tf.train.GradientDescentOptimizer(0.7).minimize(total_loss)

# L1 Regularization
l1_regularizer = tf.contrib.layers.l1_regularizer(scale, scope)
architecture_weights = tf.trainable_variables()
regularization_l1term = tf.contrib.layers.apply_regularization(l1_regularizer,architecture_weights)
total_loss = loss + regularization_l1term  
Train_step = tf.train.GradientDescentOptimizer(0.05).minimize(total_loss)
```

### Dropout

For TensorFlow a dropout layer is added directly into the computational graph.

**TensorFlow**: [documentation](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)
```python
tf.nn.dropout(
    x,
    keep_prob,
    noise_shape=None,
    seed=None,
    name=None
)
```

-->




