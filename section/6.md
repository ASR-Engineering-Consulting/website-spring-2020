---
# Page settings
layout: default
keywords:
comments: false

# Hero section
title: Section 6
description: In this section we will (1) explore the math behind L1 and L2 regularization then (2) go through how you can implement classic regularization methods in code.  


# Micro navigation
micro_nav: true

---



# L1 and L2 Regularization

We are familiar with how $$L_1$$ regularization encourages sparse weight matrices and $$L_2$$ regularization encourages small weight matrices, but **why exactly does this happen?** 

Lets consider some cost function $$J(w_1,\dots,w_l)$$ that is a function of weight matrices $$w_1,\dots,w_l$$. Define the following two regularized cost functions:


$$\begin{align} 
J_{L_1}(w_1,\dots,w_l) &= J(w_1,\dots,w_l) + \lambda\sum_{i=1}^k|w_i|\\
J_{L_2}(w_1,\dots,w_l) &= J(w_1,\dots,w_l) + \lambda\sum_{i=1}^k||w_i||^2
\end{align}$$

The update for $$w_i$$ when using $$J_{L_1}$$ is:

$$w_i^{k+1} = w_i^{k} - \underbrace{\alpha\lambda sign(w_i)}_{L_1 \text{ penalty}} - \alpha\frac{\partial J}{\partial w_i}$$


The update for $$w_i$$ when using $$J_{L_2}$$ is:

$$w_i^{k+1} = w_i^{k} - \underbrace{2\alpha\lambda w_i}_{L_2 \text{ penalty}}- \alpha\frac{\partial J}{\partial w_i}$$


**What do you notice that is different between these two update rules? Will these differences effect the optimal weight parameters? What effect does $$\lambda$$ have?**


Let's [**visualize**](https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html) this in action!

{% include image.html description="A histogram of weight values for an unregularized (red) and L1 regularized (blue left) and L2 regularized (blue right) network." link="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html" image="section/6/histogram.jpeg" caption="true"%}


The different effects of $$L_1$$ and $$L_2$$ regularization on the optimal parameters are an artifact of the different ways in which they change the original loss landscape. In the case of two parameters ($$w_1$$ and $$w_2$$) this change can be easily [**visualized**](https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html). 

{% include image.html description="The landscape of a two parameter loss function with L1 regularization (left) and L2 regularization (right)." link="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html" image="section/6/loss.jpeg" caption="true"%}

For linear regession it can be shown that $$L_1$$ regularization is equivelent to minimizing the original regression loss under a linear constraint of the parameters and $$L_2$$ regularization is equivelent minimizing the original regression loss under a quadratic constraint of the parameters.  The optimal parameters will occur at the boundary of these constraint.  Under this reformulation we can more easily understand the relationship between $$L_1$$ and sparsity and $$L_2$$ and shrinkage.  To read more about this topic look through this longer [blog post](https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html) and chapter 3 of [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/).



# Regularization in Code

### Data Augmentation

**TensorFlow**: [documentation](https://www.tensorflow.org/api_docs/python/tf/image/)
```python
# Resizing
tf_resize = tf.image.resize_images(
    images,
    size,
    method=ResizeMethod.BILINEAR,
    align_corners=False,
    preserve_aspect_ratio=False
)
# Scaling
tf_scaling = tf.image.crop_and_resize(
    image,
    boxes,
    box_ind,
    crop_size,
    method='bilinear',
    extrapolation_value=0,
    name=None
)
# Translation
tf_translation = tf.image.extract_glimpse(
    input,
    size,
    offsets,
    centered=True,
    normalized=True,
    uniform_noise=True,
    name=None
)
# Rotation
tf_90rotation = tf.image.rot90(
    image,
    k=1,
    name=None
)
# Execute Transformation
sess.run(transformation, feed_dict = {X: imgs})
```

### Early Stopping

Early stopping can be implemented directly in Python.

```python
def early_stopping(model, X_train, Y_train, X_dev, Y_dev, eval_freq = 1, stop_wait = 100):
    """
    Python implementation of the early stopping algorithm.

    Arguments:
    model -- Your model class which comes with functions:
                model.train() -- trains the model for one step and returns the updated model.
                model.eval() -- evaluates the model on a set and returns the error.
                model.copy() -- copies the model.
    X_train -- Training set
    Y_train -- Training labels
    X_dev -- Dev set
    Y_dev -- Dev labels
    eval_freq -- integer, ratio of training step per evaluation step.
    stop_wait -- number of train step without dev error improvement before stopping.

    Returns:
    best_model -- the model with minimal dev error and early stopping
    best_dev_error -- the best dev error achieved by best_model
    best_model_index -- the best_model's saving step 
    """

    current_model = model.copy()       # Track your current model
    best_model = model.copy()          # Track your best model
    num_train_steps = 0                # number of training steps
    num_steps_since_best_model = 0     # number of training steps since the last best model was saved
    best_dev_error = np.inf            # the best dev error achieved so far (by best_model)
    best_model_index = 0               # the best_model's saving step 

    # Loop unless there's no model improvement on the dev set for stop_wait steps.
    while num_steps_since_best_model < stop_wait:

        # Train current_model for "train_per_eval_ratio" steps
        for s in range(train_per_eval_ratio):
            current_model.train(X_train, Y_train)
            num_train_steps = num_train_steps + 1

        # Evaluate your current_model on the dev set.
        dev_error = current_model.eval(X_dev, Y_dev)

        # If the dev error is lower than the best dev error previously achieved, then save the current_model as the best_model
        if dev_error < best_dev_error:
            num_steps_since_best_model = 0
            best_model = current_model.copy()
            best_model_index = num_train_steps
            current_model = best_model
        # Otherwise increment num_steps_since_best_model
        else:
            num_steps_since_best_model = num_steps_since_best_model + 1

  return best_model, best_dev_error, best_model_index
```

### L1 and L2 Regularization

In TensorFlow L1/L2 regularization terms are added directly to the loss function.

**TensorFlow**: [documentation]()
```python
# L2 Regularization
regularization_terms = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2)
total_loss = tf.reduce_mean(loss + beta * regularization_terms)
optimizer = tf.train.GradientDescentOptimizer(0.7).minimize(total_loss)

# L1 Regularization
l1_regularizer = tf.contrib.layers.l1_regularizer(scale, scope)
architecture_weights = tf.trainable_variables()
regularization_l1term = tf.contrib.layers.apply_regularization(l1_regularizer,architecture_weights)
total_loss = loss + regularization_l1term  
Train_step = tf.train.GradientDescentOptimizer(0.05).minimize(total_loss)
```

### Dropout

For TensorFlow a dropout layer is added directly into the computational graph.

**TensorFlow**: [documentation](https://www.tensorflow.org/api_docs/python/tf/nn/dropout)
```python
tf.nn.dropout(
    x,
    keep_prob,
    noise_shape=None,
    seed=None,
    name=None
)
```






