---
# Page settings
layout: default
keywords:
comments: false

# Hero section
title: Section 7
description: In this section you will (i) go through techniques for hyperparameter tuning and (ii) set up an AWS instance for cloud computing.  


# Micro navigation
micro_nav: true

---


# Hyperparameter Tuning

Lots of hyperparameters are involved in the design of a deep neural network. **What are some examples?**


Finding the best set of hyperparameters for a task creates a second optimization challenge.  Here are a couple strategies for solving this problem.

### Random Search and Grid Search


Consider the following function $$f(x,y) = g(x) + h(y)$$ over parameters $$x,y$$ and the maximization problem:

$$\max_{x,y} f(x,y).$$

Assume you only have access to $$f(x,y)$$ through an *oracle* (i.e. you can evaluate $$f$$ at a certain point $$(x,y)$$, but you do not know the functional form of $$f$$).  **How would you find the optimal values of $$x$$ and $$y$$?** 

<!--  - Choose a range for the values of $$x$$ and $$y$$  and sample a grid of points in this range.
 - Evaluate a numerical gradient in the hyperparameter space and use this to inform the choice of values for $$x$$ and $$y$$.  The challenge with this method is that unlike an iteration of model training, each evaluation of hyperparameters is very costly making it infeasible to try many combinations of hyperparameters. -->

Assume that you know, 

$$f(x,y) = g(x) + h(y) \approx g(x).$$

**Would grid search still be a good strategy?  If not can you modify it to improve the results?**

<!-- - The function $f$ mostly depends on $$x$$. Thus, a grid search strategy will waste a lot of iterations testing for different values of $$y$$.  If you have a finite number of evaluations of $$(x,y)$$, then a better strategy would be randomly sampling  $$x$$ and $$y$$ in a certain range, that way each sample tests a different value of each hyperparameter. -->

{% include image.html description="An illustration of how random search can improve on grid search of hyperparameters.  'This failure of grid search is the rule rather than the exception in high dimensional hyper-parameter optimization' (Bergstra & Bengio, 2011)." link="http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf" image="section/7/random-grid.png" caption="true"%}

**What is a weakness or assumption of random search?**

<!-- - Random search assumes that the hyperparameters are uncorrelated (for example, batch size and learning rate are positively correlated: a smaller batch size generally implies a smaller learning rate). Ideally, you would sample hyperparameters from a joint distribution that takes into account this understanding.  Additionally, it does not use the results of previous iterations to inform how you choose samples for the future iterations.  This is the motivation behind Bayesian optimization stratgies. -->

### Bayesian Optimization

Bayesian inference is a form of statistical inference that uses Bayes' Theorem to incorperate prior knowledge of paramters when performing estimation.  Bayes' Theorem is a simple, yet extermely powerful, formula realating conditional and joint distributions of random variables.  Let $$M$$ be the random variable representing the quality of our model and $$\theta$$ the random variable representing our hyperparameters.  Then Bayes rules relates the distributions $$p(\theta \mid X)$$ (posterior), $$p(X\mid\theta)$$ (likelihood), $$p(\theta)$$ (prior) and $$p(X)$$ (marginal) as:

$$p(\theta\mid M) = \frac{p(M \mid \theta)p(\theta)}{p(M)}$$

**How could you use Bayes' Rule to improve random search?**
<!-- - By using a prior on our hyperparameters you can incorperate prior knowldege about the relationship between hyperparameters.  By sampling from the posterior distribution instead of a uniform joint distribution you can incorperate the results of our previous samples to improve our search process. -->

Lets reconsider the optimization problem of finding the maximum of $$f(x,y)$$.  A Bayesian optimization strategy would: 

  - Initialize a prior on the parameters $$x$$ and $$y$$. 
  - Sample an initial point $$(x,y)$$ to evaluate $$f$$ with.  
  - Use the result of $$f(x,y)$$ to update the posterior of $$x,y$$.
  - Repeat the last two steps.

Here is a good [code base](https://github.com/fmfn/BayesianOptimization) for implementing Bayesian Optimization with Gaussian processes.


{% include image.html description="With each iteration 'the algorithm balances its needs of exploration and exploitation' (Nogueira)." link="https://github.com/fmfn/BayesianOptimization" image="section/7/bayesopt.gif" caption="true"%}


# Cloud Computing Resources

Hyperparameter tuning involves retraining a model for each combination of hyperparameters chosen by one of the search algorithms presented in the previous section. Depending on the number of parameters and size of the training dataset, retraining a model can be very costly.  Thus, in order to perform a reasonable hyperparameter search you must increase the efficiency of training.

The vectorized formulas of backpropagation, derived in a previous section,  can be efficiently executed by certain hardwares such as GPUs and TPUs.  By parallelizing certain computations the appropriate hardware can speed up training by a factor of up to 100x.

AI practitioners must know how to work with remote resources in order to access the right hardware. In this discussion section, you will learn how to set up your own AWS instance.



### Setting up an AWS Instance

- Create an account [here](https://portal.aws.amazon.com/billing/signup?nc2=h_ct&src=default&redirect_url=https%3A%2F%2Faws.amazon.com%2Fregistration-confirmation#/start).
- Sign in into your account [here](https://signin.aws.amazon.com/signin?redirect_uri=https%3A%2F%2Fconsole.aws.amazon.com%2Fconsole%2Fhome%3Fstate%3DhashArgs%2523%26isauthcode%3Dtrue&client_id=arn%3Aaws%3Aiam%3A%3A015428540659%3Auser%2Fhomepage&forceMobileApp=0).
- In the home page, on the top right corner locate the region tab and set it to North Virginia (US East). This region has P2 GPU instances. 
{% include image.html description="" link="" image="section/7/region.png" caption="false"%}
- After selecting the region, click on EC2 under the Compute list.
{% include image.html description="" link="" image="section/7/compute.png" caption="false"%}
- On the next Dashboard view, click on the “Launch Instance” blue button.
- Search and select the **Deep Learning AMI (Ubuntu) Version 21.2**. This AMI (Amazon Machine Image) comes with a bunch of pre-installed deep learning frameworks such as TensorFlow, PyTorch or Keras. 
- In the next page, select the p2.xlarge instance and click on “Review and Launch”.
{% include image.html description="" link="" image="section/7/review.png" caption="false"%}
- Then, click on the blue button “Launch”.
- A pop-up window will appear asking for a key pair. You can either provide one or create one. If you create one, you should download it and keep it somewhere it won’t be deleted (if that happens, you won’t be able to access your instance anymore!).
- If you downloaded the key file, change its permissions in the terminal to user-only read and write. In Linux, this could be done with `chmod 600 PEM_FILENAME` where PEM_FILENAME is the file with the key.
- After this, click on the blue button “Launch Instances”. 
- Click on “View Instances” to check that it is “Running” and passed “2/2 status checks”. It will take some time to pass the checks but after that, you will be ready to ssh into the instance. Finally, note down the Public IP of the instance launched (it will be required in the next step).
- SSH in your instance with `ssh -i PEM_FILENAME ubuntu@PUBLIC_IP`.









