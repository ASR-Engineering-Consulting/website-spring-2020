---
# Page settings
layout: default
keywords:
comments: false

# Hero section
title: Section 2
description: A quick walkthrough of how you should start your project. Given a project idea, how do you go from the idea to an implementation, how do you structure your search for good code bases to build on, and how do you quickly get a baseline model running locally.  

# Micro navigation
micro_nav: true

---

# Running style transfer locally

{% include image.html description="Le musée du Louvre in Impressionist style from Ng, Katanforoosh & Bensouda." link="https://www.deeplearning.ai/deep-learning-specialization/" image="section/2/styletransfer.png" %}

  * **You are working on a project entitled “Improving Neural Style Transfer”. If you were to plan your work, what would be the steps to carry-out?**

Here is our recommended list of steps for your deep learning project:

(1) *Find project partners and project idea* (see discussion [section 1](/section/1).)

(2) *Conduct and in-depth literature and implementation review.* If possible, seek help from experts to get a good selection of resources. Use a Broad-to-Specific approach by first giving a quick look at many papers/implementations and then focusing on a subset of them that seem the most relevant to your task.

(3) *Find or collect a labelled dataset.* Depending on your project, this might include cleaning and preprocessing.

(4) *Implement a baseline model.*
  - If there exists implementations of your task, try to reproduce them by running the open-source code.
  - If there is no implementation of your task, find a proxy task that has been solved and plug in your dataset. For example, if you want to build a fish breed classifier, you can find open-source code for a cat breed classifier and plug in your fish breed dataset.
  - If your task hasn’t been tackled before and there is no proxy, build a first simple algorithm that solves the problem without aiming at a great performance. 
  - Oftentime, you will notice that the algorithm struggles at predicting the right output on your dataset. One thing you could try is to focus on overfitting a very small number of examples, and gradually adding more and more. By focusing on a few examples you can run myriads of experiments on your architecture and hyperparameters in a short amount of time. . When your model starts overfitting a good amount of training examples, , analyze the algorithm’s generalization capabilities on unseen data.

(5) *Improvement of the baseline model.* This includes hyperparameter tuning, architecture search and other ideas to obtain a superior model. Define and use a metric to compare your upgraded models to the baseline.

(6) *Final write-up.* report in a clear and concise way your findings in steps 4 and 5.
 
  * **As first step in your project’s implementation, you’d like to learn about existing implementations. Search online and choose a github repository that you think would be good to study.**
 
  * **What made you choose this github repository?**

Here’s a framework to efficiently filter content in Github:

(1) *How many stars does the Git repository have?* In general, the number of stars is a good indicator of the quality of the content. 

(2) *What are the licenses for the Git repository’s code and the dataset?*  Code projects falling under MIT, Apache, GNU, BSD licenses are open-source. Most of the time, it means that you can use the code as you want (some time, you will need to paste the same license in your own project.) . For datasets, you mostly want to look at CC or CC-BY licenses which allow you to use the data for your project.

(3) *Is the ReadMe well furnished?* Are results reported? *Is it possible to quickly setup the repository locally?* Good repositories provide instructions in the README to run the code. It also provides results.

(4) Does the Git repository have links to  download compatible datasets and ideally, pretrained model weights?

(5) Is the code written in your preferred deep learning framework? (Tensorflow, Pytorch, Keras, etc.)

(6) *How’s the code’s quality and readability?* Check that the code is commented and the repository folders are well-structured.

(7) *Does the Git repository provide references?* You can usually find those at the bottom of the README.
 
  * **The github repository [neural-style](https://github.com/anishathalye/neural-style) seems to satisfy our criteria, let’s focus on it. Take 5 minutes to read and run the github repository on its inherent data. You should see the message “Iteration .../1000”. What challenges did you encounter?** 

Each single iteration takes a long time. Therefore, it looks like it is not possible to rapidly validate the decreasing shape of the loss function locally. 
 
  * **It seems that a few minutes is not enough to reproduce good results locally, what can you tweak to overcome this challenge?**
 
Recall that the goal at this stage is just to make sure that the repository’s cloned code from Github is working. Assuming you don’t have GPU resources at the moment, a quick solution would be to downsize or crop the images. 

  * **Run this code on your chosen content and style images. Use 100 iterations.**


This section was designed to help you get started with the existing code bases related to your project. Note that we only tested the model here, and did not train it. At home, find a github repository that is relevant to your project and try to reproduce the results. Also, if you need a refresher on terminal or git fundamentals, you can have a look at this [tutorial](https://d1b10bmlvqabco.cloudfront.net/attach/jd9nm7v2drf4bz/j7v7aqkb9194qv/jgic5sc45imn/CS230GithubTutorial.pdf).


# Further reading

If you’re interested in the style transfer problem, here are some interesting papers for you to read through:
 * [A Neural Algorithm of Artistic Style](https://arxiv.org/pdf/1508.06576.pdf), Gatys et al. (2015) 
 * [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/pdf/1603.08155.pdf), Johnson et al. (2016)
 * [A learned representation for artistic style](https://arxiv.org/pdf/1610.07629.pdf), Dumoulin et al. (2016)
 * [Demystifying Neural Style Transfer](https://arxiv.org/abs/1701.01036), Li et al. (2017)
 * [Exploring the structure of a real-time, arbitrary neural artistic stylization network](https://arxiv.org/pdf/1705.06830.pdf), Ghiasi et al. (2017)
 * [Neural Artistic Style: a Comprehensive Look](https://medium.com/artists-and-machine-intelligence/neural-artistic-style-transfer-a-comprehensive-look-f54d8649c199), Desai (2017)




