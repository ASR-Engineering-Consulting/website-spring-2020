---
# Page settings
layout: default
keywords:
comments: false

# Hero section
title: Section 4
description: Xavier Initialization and Regularization


# Micro navigation
micro_nav: true

---

Coming soon!

<!--

Before getting started:
  - Open either the [**TensorFlow Tutorial**](https://colab.research.google.com/drive/1HzN2f0Mypj0r2rKJdKYCjczM1WzJyoaV) or the [**Pytorch Tutorial**](https://colab.research.google.com/drive/1a2KshOZVj4eqYsfFqlBHB66CUIERKHj5), go to "File", and press "Save a copy in Drive...".
  - Download [**data.zip**](https://drive.google.com/file/d/1K2hSiWiufAglL8MoTrWdStuVA2WjiTQa/view), upload the .zip file into the notebook by opening side menu (black arrow >), navigate to "Files" submenu, and press "Upload".

# Basic Pipeline 
In this part, we’ll walk through a end-to-end pipeline for a classification task on the MNIST dataset. The MNIST dataset contains images of handwritten digits that are 28x28 with labels 0-9 and can be loaded directly from Tensorflow.

{% include image.html description="Example images from MNIST dataset." link="https://commons.wikimedia.org/wiki/File:MnistExamples.png" image="section/4/mnist.png" caption="true"%}

  - **What are the steps for implementing an end to end pipeline?**

The steps for implementing an end to end pipeline are:

  (1) Load the dataset

  (2) Define the model

  (3) Define the loss function and optimizer

  (4) Define the evaluation metric

  (5) Train the network on the training data
  
  (6) Report results on the train and test data (using the evaluation metric)


# Custom Dataloader

Most class projects use a dataset that is not available in Tensorflow/Pytorch. Let’s see how we can modify our pipeline to read in a custom dataset. We will do this by building a data loader object that will return a batch of (image, label) pairs on each call.

For this part, we will work with a sample of the SIGNS dataset. The dataset contains images of hand signs depicting numbers 0,1,2,3,4 and 5.
{% include image.html description="Example images from SIGNS dataset." link="https://www.coursera.org/learn/deep-neural-network/" image="section/4/signs.png" caption="true"%}

  - **Why do we need a data loader object? Can’t we just load the entire dataset in memory to use for training and testing?**

Most datasets used for deep learning are too big to fit entirely in memory. Thus, we need a data loader object that reads in a subset of the data in memory as necessary.

  - **Each image has a corresponding label (the number being depicted). What are some ways to store this mapping between images and labels?**

Some options (not an exhaustive list):

  (1) Store the label as part of the file name (eg; 0_image1.png, 2_image12.png, etc)

  (2) Create different folders on disk for a particular label and put all images with that label in the folder (eg; all images with label 0 are placed in a folder called ‘Label_0’)

  (3) Create a csv with a columns that store paths to the images and the corresponding labels. eg: A csv file that looks like this -
Path,Label
Image1.png,1
Image2.png,0

In this section, we will use the third option (storing the mapping in a csv file) and create separate csv files for the train, dev and test set (‘train.csv’, ‘valid.csv’, ‘test.csv’). This option has a couple of advantages:

  (1) You can easily change your train/dev/test split by modifying the csv files. If you had folder on file storing the images (i.e. a folder for training set, a folder for dev set etc), you’d have to change the underlying directory structure every time you had to re-split the data. 

  (2) If you want to exclude images with a certain label, you only need to modify the csv files.

  (3) If you need to modify the labels (eg; combine two label classes), it’s straightforward to do.

The disadvantage of this approach is that it is slow because you load in a batch of images only when you need them and you need to repeat this multiple times for a single epoch. Moreover, if there are certain transforms that you apply to the images (eg; crop/resize), you need to apply the transforms for every single epoch.

An alternative is to apply the transforms to the images once and store the transformed images on disk as a ‘.npz’ file (compressed numpy array). You can then include the paths to the transformed images in the csv instead of the path to the original images.




# Saving/Loading a model
Lastly, we’ll go over how to save and load a model in Tensorflow. You’ll need to save models from every experiment you run, so that you can then load the best model to evaluate on the test set. 

-->



