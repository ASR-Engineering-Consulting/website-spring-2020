---
# Page settings
layout: default
keywords:
comments: false

# Hero section
title: Section 4
description: Xavier Initialization and Regularization


# Micro navigation
micro_nav: true

---

# Xavier Initialization

Last week, we discussed backpropagation and gradient descent for deep learning models. All deep learning optimization methods involve an initialization of the weight parameters. **Does the initialization choice affect training?**

Explore the first visualization in this [article](https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/initialization/index.html) to gain intuition on the effect of the initialization. **What makes a good initialization?  Or rather what makes a bad initialization?** 

An initialization that is "too large" leads to the exploding gradient problem, while an initialization that is "too small" leads to the vanishing gradient problem.

{% include image.html description="A visualization for the exploding and vanishing gradient problem in a deep linear network." link="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/initialization/index.html" image="section/5/exploding-vanishing.png" caption="true"%}

The **goal** of Xavier Initialization is to find an initialization such that the variance of the activations are the same across every layer to prevent the signal from exploding or vanishing. To achieve this, we will make the following simplifying **assumptions**:

  * Weights and inputs are distributed around zero, and biases are zero
  * Weights are independent and identically distributed
  * Inputs are independent and identically distributed
  * Weights and inputs are mutually independent

Xavier Initialization is the following initialization rule:

$$W^{[l]}_{i,j} = \mathcal{N}\left(0,\frac{1}{n^{[l-1]}}\right)$$

Let's show how this initialization leads to constant variance between layers.

{% include image.html description="" link="" image="section/5/proof.png" caption="false"%}

Check out this [article](https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/initialization/index.html) to read more about initializations in deep learning.


# L1 and L2 Regularization

We know that $$L_1$$ regularization encourages sparse weights (many zero values) and $$L_2$$ regularization encourages small weight values, but **why does this happen?** 

Let's consider some cost function $$J(w_1,\dots,w_l)$$, a function of weight matrices $$w_1,\dots,w_l$$. Let's define the following two regularized cost functions:

$$\begin{align} 
J_{L_1}(w_1,\dots,w_l) &= J(w_1,\dots,w_l) + \lambda\sum_{i=1}^k|w_i|\\
J_{L_2}(w_1,\dots,w_l) &= J(w_1,\dots,w_l) + \lambda\sum_{i=1}^k||w_i||^2
\end{align}$$

The update for $$w_i$$ when using $$J_{L_1}$$ is:

$$w_i^{k+1} = w_i^{k} - \underbrace{\alpha\lambda sign(w_i)}_{L_1 \text{ penalty}} - \alpha\frac{\partial J}{\partial w_i}$$


The update for $$w_i$$ when using $$J_{L_2}$$ is:

$$w_i^{k+1} = w_i^{k} - \underbrace{2\alpha\lambda w_i}_{L_2 \text{ penalty}}- \alpha\frac{\partial J}{\partial w_i}$$


**What do you notice that is different between these two update rules? Will these differences effect the optimal weight parameters? What effect does $$\lambda$$ have?**

Let's [**visualize**](https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html) this in action!

{% include image.html description="A histogram of weight values for an unregularized (red) and L1 regularized (blue left) and L2 regularized (blue right) network." link="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html" image="section/6/histogram.jpeg" caption="true"%}

The different effects of $$L_1$$ and $$L_2$$ regularization on the optimal parameters are an artifact of the different ways in which they change the original loss landscape. In the case of two parameters ($$w_1$$ and $$w_2$$), this change can be easily [**visualized**](https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html). 

{% include image.html description="The landscape of a two parameter loss function with L1 regularization (left) and L2 regularization (right)." link="https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html" image="section/6/loss.jpeg" caption="true"%}

For linear regession it can be shown that $$L_1$$ regularization is equivalent to minimizing the original regression loss under a linear constraint of the parameters and $$L_2$$ regularization is equivelent minimizing the original regression loss under a quadratic constraint of the parameters.  The optimal parameters will occur at the boundary of these constraint.  Under this reformulation we can more easily understand the relationship between $$L_1$$ and sparsity and $$L_2$$ and shrinkage.  To read more about this topic look through this longer [blog post](https://rawgit.com/danielkunin/Deeplearning-Visualizations/master/regularization/index.html) and chapter 3 of [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/).

