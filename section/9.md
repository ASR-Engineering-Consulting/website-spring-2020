---
# Page settings
layout: default
keywords:
comments: false

# Hero section
title: Section 9
description: This section goes through an ovierview of evaluation metrics for different tasks in deep learning.  


# Micro navigation
micro_nav: true

---


# Predictive Models

The choice of the correct evaluation metric will highly depend on the the task of your model.  Broadly speaking, predicitve neural networks will either be in a regression or classification setting.

<!-- ## Regression Metrics

### Mean Square Error (MSE)
### Mean Absolute Error (MAE) -->


## Classification Metrics

Lets consider a simple binary classification problem where you are trying to predict if a patient is healthy or if the patient has pneumonia. You have a test set with 10 patients where 9 patients are healthy (shown in green squares) and 1 patient has pneumonia (shown in red square).

{% include image.html description="Ground truth for your test set." link="" image="section/9/truth.png" caption="true"%}

You have trained 3 models on this task (Model1, Model2, Model3) and you’re trying to compare the performance of these models. The predictions from each model on your test set is shown below (green - model predicted patient to be healthy, red - model predicted patient to have pneumonia). 

{% include image.html description="Results of your three models on the test set." link="" image="section/9/models.png" caption="true"%}


### Accuracy

To compare your models you first use accuracy, which is simply the number of correct classified test samples divided by the total number of test samples:

$$\text{accuracy}(f) = \frac{\sum_{x_i \in X_{test}} \mathbb{1}\{f(x_i) = y_i\}}{\mid X_{test} \mid}$$

**What is the accuracy for each model? Do you think this is the correct metric to use?**

<!-- $$\text{Accuracy}(M_1) = \frac{}{10} \qquad \text{Accuracy}(M_2) = \frac{}{10} \qquad \text{Accuracy}(M_3) = \frac{}{10}$$ -->

<!-- If you use accuracy as your evaluation metric, it seems that the best model is Model1. In general, when you have class imbalance (more samples from one class than other classes), accuracy is not a good metric to use. -->

### Confusion Matrix

Accuracy doesn't descriminate between errors (i.e. it treats misclassifying a patient with pneumonia as healthy the same as misclassifying a healthy patient with having pneumonia).  A confusion matrix is a tabular format for showing a more detailed breakdown of a model's correct and incorrect classifications.

{% include image.html description="A confusion matrix for binary classification." link="" image="section/9/confusion.png" caption="true"%}

**Can you find a simple formula for calculating accuracy from a confusion matrix?**

### Precision, Recall, F1 Score

In the example of pneumonia detection, it is crucial that you find all the patients that have pneumonia. Predicting patients with pneumonia as healthy can have disastrous consequences (since the patients will be left untreated). Thus, a natural question to ask when evaluating your models is:

*Out of all the patients with pneumonia, how many did the model predict as having pneumonia?*

This metric is called recall.  In terms of the elements of the confusion matrix it can be expressed as

$$\text{Recall} = \frac{tp}{tp + fn}$$

**What is the recall for each model?**

<!-- $$\text{Recall}(M_1) = \frac{}{1} \qquad \text{Recall}(M_2) = \frac{}{1} \qquad \text{Recall}(M_3) = \frac{}{1}$$ -->

Imagine that the treatment for pneumonia is very costly and therefore you would also like to make sure only patients with pneumonia recieve treatment.  A natural question to ask would be: 

*Out of all the patients that are predicted to have pneumonia, how many actually have pneumonia?*

This metric is called precision.  In terms of the elements of the confusion matrix it can be expressed as

$$\text{Precision} = \frac{tp}{tp + fp}$$

**What is the precision for each model?**

<!-- $$\text{precision}(M_1) = \frac{}{1} \qquad \text{precision}(M_2) = \frac{}{1} \qquad \text{precision}(M_3) = \frac{}{1}$$ -->

Based on the recall and precision for each model can we now determine which is the best model?

*“Having multiple-number evaluation metrics makes it harder to compare algorithms. Better to combine them to a single evaluation metric. Having a single-number evaluation metric speeds up your ability to make a decision when you are selecting among a large number of classifiers. It gives a clear preference ranking among all of them, and therefore a clear direction for progress.” - Machine Learning Yearning*

F1 score is a metric that combines recall and precision by there harmonic mean

$$F_1 = 2 \cdot \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{recall}}$$


**What is the F1 score for each model?**

<!-- $$F_1(M_1) = \frac{}{} \qquad F_1(M_2) = \frac{}{} \qquad F_1 = \frac{}{}$$ -->


<!-- ### Area Under the Curve (AUC) -->




# Generative Models

GAN loss functions usually measure how well the generator fools the discriminator. That is, the generator weights are optimized to maximize the probability that any fake image is classified by the discriminator as belonging to the real dataset. 

However, GAN loss functions don’t measure the quality and diversity of the images generated. Thus, in order to benchmark GANs based on these two image criterions, we would need a new metric or procedure to compare them. 

## Inception Score

As mentioned above, we want to define a metric that returns a high score to GANs that output diverse and good quality images.

**What quantity could you maximize to check that the generated images are of high quality?**

One data driven way of making sure that the generated images are of high quality is by observing that is easy to infer the class of a high quality image with a NN classifier due to its high resolution (content of information) and their visual correspondence to the desired class (saliency). Mathematically, this is equivalent to say that the probability distribution of the class conditioned over an input image $$p(y\mid x)$$ has low entropy (highly spiked, concentrated). Hence the name of the metric: for estimating $$p(y\mid x)$$ the authors used a pretrained Inception network.

**How could you penalize GAN generators that don’t output diverse images?**

Diversity means having images from all the classes we are interested in. Using again the same statistical feature (the entropy), we can formulate mathematically a new figure of merit to award diversity in the generated images. How? By forcing the distribution of classes $$p(y)$$ of the generated dataset to have a high entropy. 

**How can be the entropy of $$p(y)$$ computed?** 

As we can estimate $$p(y\mid x)$$ with an Inception NN, $$p(y)$$ would be its marginal distribution.  As $$x = G(z)$$, $$p(y)$$ can be derived integrating over $$z$$ as follows:

$$\int_z p(y\mid x = G(z)) dz$$

**How you would combine in a single metric the previous two criterions?**

Using the KL divergence. Intuitively, the KL divergence can be thought as the “distance” in terms of similarity between two distributions.

The Inception score (of a Generator) is defined as follows:

$$IS(G) = exp(\mathbb{E}_{x \sim p_\theta} D_{KL}(p(y\mid x) \mid\mid p(y)))$$


If the images produced by the generative model are diverse and of high quality (i.e. $$p(y\mid x)$$ has low entropy and $$p(y)$$ has high entropy), then we expect a large KL-divergence between the distributions $$p(y)$$ and $$p(y\mid x)$$, resulting in a large IS. 

One shortcoming for IS is that it can misrepresent the performance if it only generates one image per class. $$p(y)$$ will still be uniform even though the diversity is low.


## Frechet Inception Distance

This metric compares the statistics of the generated samples and real samples. It models both distributions as multivariate Gaussian. Thus, these two distributions can be compactly represented by their mean $$\mu$$ and covariance matrix $$\Sigma$$ exclusively. That is:

$$X_r\sim N(\mu_x,\Sigma_x) \text{ and } X_g\sim(\mu_g,\Sigma_g)$$

These two distributions are estimated with 2048-dimensional activations of the Inception-v3 pool3 layer for real and generated samples respectively. 

Finally, the FID between the real image distribution ($$Xr$$) and the generated image distribution ($$Xg$$) is computed as:

$$FID(x,g) = ||\mu_x - \mu_g||_2^2 + Tr(\Sigma_x + \Sigma_g - 2(\Sigma_x\Sigma_g)^{\frac{1}{2}})$$

Therefore, lower FID corresponds to more similar real and generated samples as measured by the distance between their activation distributions. 




# Task Specific Metrics

- Object Detection
  - Intersection over Union (IoU)
  - Mean Average Precision (maP)
- Machine Translation
  - Bilingual Evaluation Understudy (BLEU) Score










