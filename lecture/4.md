---
# Page settings
layout: default
keywords:
comments: false

# Hero section
title: Lecture 4
description: Add short description.

# Micro navigation
micro_nav: false

---

# Lecture 4: Adversarial Attacks and GANs

In this lecture, we will have an overview of recent studies on adversarial examples and GANs. We briefly introduce these two important subjects in deep learning, and discuss several methods and applications centered on this topics. In the first half of this lecture, we get to know adversarial examples and their potential dangers in deep learning settings. Next we move to GANs and how we can generate fresh samples via GANs.

## Adversarial examples and attacks
### Introduction
In 2013, Szegedy and his team discovered that neural networks can be prone to vulnerabilities. It turns out that some machine learning models are vulnerable to forged examples. This phenomenon can be seen in various fields, including with:
- text
- speech
- images

We will focus these notes on the latter part. To visualize this phenomenon, an example can be that we can make a network believe with high probability that an example is not a cat, whereas what it really is, is a cat.

Another way to generate an adversarial example is to start from a noisy input and make the model believe with a high probability that it belongs to a certain class, whereas the corresponding image does not look like anything in particular.

These observations have deep consequences on the way we perceive neural networks. In the example, of self-driving cars, imagine that a small perturbation of the network could lead to make it believe that a 'STOP' sign is in fact a '70 miles' speed limit. This could lead to serious trouble in a real-life setting.

### Attacking a network with adversarial examples
#### General procedure
Given a pre-trained model and two classes $i$ and $j$, the following framework is a quick way to make a class $i$ sample that looks like it comes from class $j$ but looks just the same from an human eye.

1. Pick a sample $X$ that is classified with strong probability as belonging to class $i$.
2. Define a loss function that quantifies the distance between the current prediction of the class of the image and the target class $j$. In other words, one possible function can be:
$L(\hat{y}, y_j) = \frac{1}{2}||\hat{y} - y_j||^2$
3. By keeping the weights of the neural network frozen, perform gradient descent to find a $\hat{X}$ that is predicted as belonging to class $j$.

Now, the solution to that optimization might be any image of the input space. We can however alleviate that issue by adding a term to the loss that translates how close the iterations of the image $\hat{X}$ are to the initial image $X$. One way of doing so can be done by including an $L_2$ distance term between the pixels of the initial input and those of the current iteration of the image. In other words, we can consider the following extra term:
$L(\hat{X}, X) = \lambda \cdot ||\hat{X} - X||^2$

where $\lambda$ is a hyperparameter that we can tune to balance the trade-off between the two terms of the loss expression.

That's it! After running the optimization process with this procedure, we now have an adversarial example.

#### In practice
After running the optimization, we realize that the output image is often times (after using some additional tricks such as clipping) very close to the one we started the optimization with, with changes that are barely visible to the human eye.

The underlying reason to that observation is that the space of possible input images is much larger than the space of real images. Just to give a number on that fact, there are $256^{L\times H \times 3}$ possible color images of size $L\times H$. For example, if we take $L = H = 32$, we get $10^{7400}$ possible images.

As a result, the space of possible images output by this optimization is much larger than the space of images in which we can imagine meaningful depictions of objects.

### Some remedies to counter these attacks
#### Types of attack
There are two types of attacks:
- white-box: here, the attacker has access to the network, just as we supposed it to be the case in the procedure of the previous section.
- black-box: in this case, the attacker does not have access to the network, which is what happens most of the time.

#### Solutions
The following solutions can help in countering both types of attacks:
1. Create a SafetyNet: this network's role is to identify adversarial examples by looking at images from a finer perspective.
2. Include adversarial examples in the training set: this consists of simply training the initial network with additional examples to force the representation of each class to take into account small perturbations.
3. Perform an adversarial training of the network: for each example fed to the network, the loss also takes into account the prediction of a perturbation $x_{adv}$ of the input $x$ of label $y$. In other words, by calling $L$ the 'normal' loss of the network, $W$ its parameters, a possible candidate for the new loss can be given by $L(W,x,y) + \lambda L(W,x_{adv}, y)$. Here, $\lambda$ is a hyperparameter that balances model robustness versus performance towards real data.

### Vulnerability of neural networks to adversarial examples
#### Setting
We illustrate the sensitivity of neural nets to perturbation on a very simple setting: the logistic regression.

Let's suppose that we perform a forward pass on the network defined by: $\hat{y} = \sigma{Wx + b}$, where $x \mapsto \sigma(x)$ indicates the sigmoid function.

#### A simple example
Let $W = [1, 3, -1, 2, 2, 3]$, $b = 0$ and $x = [1, -1, 2, 0, 3, -2]^T$. In this setting, we get $\hat{y} = 0.27$. This means that with $73\%$ probability, the predicted class is $0$.

Now, how can we change $x$ slightly and have a big impact on $\hat{y}$? Well, we have to look for components of $x$ that multiply high-magnitude coefficients of $W$, or in other words look for perturbations that are in the same direction as $W$.

Suppose we look at $x_\text{adv}$ defined by a small but targeted perturbation, say $x$ + $\epsilon W$ for $\epsilon = 0.2$. We get a value of $\hat{y} = 0.83$.

This small example shows that for a good choice of an $\epsilon$-perturbation, one can have a considerable impact on the output of a neural network.

#### Consequence
As a result, we can think of generating impactful adversarial examples with the simple formula
$x_\text{adv} = x + \epsilon \text{sign}(\nabla_x L(W,x,y))$

which is called the **fast gradient sign method**.


## Generative Adversarial Networks (GANs)

### Motivation

Imagine a task where we want a machine to grasp an understanding of a real world object. To do this, we can collect a lot of samples from that object, e.g. cat pictures, and then use those samples to train a model generating fresh samples of that object (e.g. generating new cat pictures). Intuitively, this task should be feasible as long as the complexity of our generative model (number of parameters) is bounded above by the number of samples. Then, our goal is to ideally match the generative model to the probability distribution of our data.

### G/D Game

One recent approach for training generative models is through generative adversarial networks (GANs). GANs are trained using a game between two players. The first player is a generator machine ($G$) whose goal is to generate real-like samples from random noise input. For example, the input to generator could be a 100-dimensional array z=[0.47,...,0.91] and we expect its output will look like a real image.

However, an untrained generator produces only noise-like output, which is very far from what we aim for. How can we successfully train the generator for generating images from the true distribution of data? To reach this goal, we add the second player, the discriminator (D), whose goal is to distinguish the generated samples from the real training data points.

To implement this game, we consider two neural net functions G and D, and run Adam optimizer simultaneously using two mini batches of real and fake samples. To update the parameters of D, we need to backpropagate gradients through the discriminator neural net to train D to identify real from fake images. On the other hand, for training the generator function the gradients should complete the backward pass through both the discriminator and generator neural nets. We hope the distribution of fake samples coming from a trained G  matches the true distribution of data after sufficiently-many iterations of gradient updates.

To formulate the game, we should have two cost functions for the generator and discriminator functions. For the discriminator cost, we can simply consider a cross-entropy loss for the binary classification of real vs. generated
$$J^{(D)} = -\frac{1}{m_{\text{real}}}\sum_{i=1}^{m_{\text{real}}} y_{\text{real}}^{(i)}\log (D(x^{(i)})) -\frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} (1-y_{\text{gen}}^{(i)})\log (1-D(G(z^{(i)}))).  $$

In this classification task, we always have $y_{\text{real}} = 1$, $y_{\text{gen}} = 0$. Since G is trying to fool D in the zero-sum game, the generator cost can be defined as negative the discriminator cost, i.e.
$$ J^{(G)} = \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (1-D(G(z^{(i)}))). $$

### Training GANs

So far we have introduced and formulated the GAN game. However, there exist important subtle points when we train GANs in practice. Here we discuss two important tricks for training GANs.

The first thing to note is that $D(G(z^{(i)}))$'s have very small near-zero initial values and hence the term $\frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} (1-y_{\text{gen}}^{(i)})\log (1-D(G(z^{(i)})))$ in the generator cost $J^{(G)}$ has small-norm gradients with respect to G's parameters in the beginning of GAN training. On the other hand, $D$'s parameters are not suffering from this issue and hence the discriminator player $D$ is learning much faster than the generator player $G$. As a result, GAN training might be very slow and unsuccessful. This phenomenon is known as the saturating cost issue.

To resolve this issue, we can slightly change the formulation of the generator cost and consider the following non-saturating cost function.
$$ J^{(G)} = - \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (D(G(z^{(i)}))).  $$
Note that both the saturating and non-saturating cost functions are inclined toward smaller near-zero $D(G(z^{(i)}))$ values for the generated samples. This new cost function in contrast does not lead to a saturating cost in the initial phase of training, since in the beginning the values for all logarithm's input is close to 0 rather than 1. So in the modified GAN training we minimize the mentioned non-saturating discriminator and generator cost functions.

![Alt text](sat_cost.png)

As another frequently-used trick, we can apply multiple $k>1$ discriminator gradient updates per each generator gradient update. The additional updates for $D$ keeps the discriminator up-to-date with respect to potentially fast changes in G.

## Examples of GAN applications and nice results

Now we quickly go over some successful applications of GANs.
### Operation on latent codes
The latent space of random noise input, from which GANs map to the real sample space, usually contains sound meanings. As an example, the following picture shows that by inputing the latent code of the sum of codes for man wearing glasses and a man without glasses minus a woman without glasses we get a picture of a woman wearing glasses.

![Alt text](latent_code.png)

### Image generation
As already discussed, GANs are usually successful in generating real-like new non-existing pictures of humans, animals, buildings, etc. Here are some interesting examples:

![Alt text](GAN_pics.png)
### Image to Image translation via Cycle-GANs

Translating images between different domains has been an important application of GANs. For example, we may want to translate a horse picture into a zebra picture. To do this, we can apply cycle GANs. Cycle GANs consist of two pairs of generator-discriminator players: (G_1,D_1), (G_2,D_2). The goal of the G_1/D_1 game is to turn domain 1 samples into domain 2 samples. In contrast, the goal of G_2/D_2 game is to turn domain 2 samples into domain 1 samples. Finally we have the cycle constraint which imposes that the composition of G_1 and G_2 results in the identity function. This constraints make sure that the other non-changing objects (non-horse or zebra elements) remain the same in the cycle GAN's translation. We can summarize this four-player game into the following five cost functions:
$$J^{(D_1)} = -\frac{1}{m_{\text{real}}}\sum_{i=1}^{m_{\text{real}}} \log (D_1(z^{(i)})) -\frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (1-D_1(G_1(H^{(i)}))),  $$

$$ J^{(G_1)} = - \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (D_1(G_1(H^{(i)}))),  $$

$$J^{(D_2)} = -\frac{1}{m_{\text{real}}}\sum_{i=1}^{m_{\text{real}}} \log (D_2(h^{(i)})) -\frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (1-D_2(G_2(Z^{(i)}))),  $$

$$ J^{(G_2)} = - \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \log (D_2(G_2(Z^{(i)}))),  $$

$$ J^{\text{cycle}} = - \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \Vert G_2(G_1(H^{(i)})) - H^{(i)} \Vert_1 - \frac{1}{m_{\text{gen}}}\sum_{i=1}^{m_{\text{gen}}} \Vert G_1(G_2(Z^{(i)})) - Z^{(i)} \Vert_1 .  $$

Here are some nice results achieved by cycle GANs:

![Alt text](cyclegan.png)

### Low-resolution to high-resolution images

One can apply GANs to get the high-resolution version of low-resolution images:

![Alt text](res_GAN.png)

### Other examples
- Beaulieu-Jones et al., "Privacy-preserving generative deep neural networks support clinical data sharing",
- Hwang et al., "Learning Beyond Human Expertise with Generative Models
for Dental Restorations",
- Gomez et al., "Unsupervised cipher cracking using discrete GANs".  
